{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Brain Tumor Segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (Un)Healthy Brain Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import cv2\r\n",
    "import urllib\r\n",
    "from PIL import Image\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import albumentations\r\n",
    "import time\r\n",
    "import argparse\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "# Pytorch libraries and subsidiaries\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.cuda as cuda\r\n",
    "import torchvision as tv\r\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\r\n",
    "from torchvision import transforms as T\r\n",
    "from torchsummary import summary # allows for keras-like summaries of models in pytorch\r\n",
    "from torchvision.utils import make_grid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check CUDA Capability and Set Device"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "if cuda.is_available():\r\n",
    "    device = torch.device('cuda:0')\r\n",
    "    print('Running on ' + cuda.get_device_name(device))\r\n",
    "else:\r\n",
    "    device = torch.device('cpu')\r\n",
    "    print('Running on the CPU')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Pretrained Models and Set to Device"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%%capture --no-stdout\r\n",
    "\r\n",
    "# Load our pre-trained models\r\n",
    "brain_seg_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\r\n",
    "class_model = tv.models.resnet50(pretrained=True)\r\n",
    "\r\n",
    "# Set our models to run on the GPU if available and the CPU if not\r\n",
    "brain_seg_model = brain_seg_model.to(device)\r\n",
    "class_model = class_model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# summary(class_model) # Get some stats on our pretrained model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# Download an example image\r\n",
    "url, filename = (\"https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/TCGA_CS_4944.png\", \"example.png\")\r\n",
    "try: urllib.URLopener().retrieve(url, filename)\r\n",
    "except: urllib.request.urlretrieve(url, filename)\r\n",
    "\r\n",
    "input_image = Image.open(filename)\r\n",
    "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))\r\n",
    "preprocess = T.Compose([\r\n",
    "    T.ToTensor(),\r\n",
    "    T.Normalize(mean=m, std=s),\r\n",
    "])\r\n",
    "input_tensor = preprocess(input_image)\r\n",
    "input_batch = input_tensor.unsqueeze(0)\r\n",
    "\r\n",
    "if torch.cuda.is_available():\r\n",
    "    input_batch = input_batch.to('cuda')\r\n",
    "    class_model = class_model.to('cuda')\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    output = class_model(input_batch)\r\n",
    "\r\n",
    "# print(torch.round(output[0]))\r\n",
    "# display(input_image)\r\n",
    "# cv2.imread(filename).shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Pandas Dataframe of Images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tumor_dir = 'brain-tumor-dataset/Brain Tumor'\r\n",
    "healthy_dir = 'brain-tumor-dataset/Healthy'\r\n",
    "dirlist = [tumor_dir, healthy_dir]\r\n",
    "classes = [1, 0] # 1 = tumor, 0 = no tumor\r\n",
    "filepaths = []\r\n",
    "labels = []\r\n",
    "for d,c in zip(dirlist, classes):\r\n",
    "    flist = os.listdir(d)\r\n",
    "    for f in flist:\r\n",
    "        fpath = os.path.join (d,f)\r\n",
    "        filepaths.append(fpath)\r\n",
    "        labels.append(c)\r\n",
    "print ('filepaths: ', len(filepaths), '   labels: ', len(labels))\r\n",
    "\r\n",
    "Fseries = pd.Series(filepaths, name='file_paths')\r\n",
    "Lseries = pd.Series(labels, name='labels')\r\n",
    "df = pd.concat([Fseries,Lseries], axis=1, names=['file_paths', 'labels'])\r\n",
    "df = pd.DataFrame(np.array(df).reshape(4600,2), columns=['file_paths', 'labels'])\r\n",
    "print(df['labels'].value_counts())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "filepaths:  4600    labels:  4600\n",
      "1    2513\n",
      "0    2087\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Splitting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_df, test_df = train_test_split(df, train_size=0.95, random_state=0)\r\n",
    "train_df, valid_df = train_test_split(train_df, train_size=0.9, random_state=0)\r\n",
    "\r\n",
    "print('Training Set:')\r\n",
    "print(train_df.labels.value_counts())\r\n",
    "print('Validation Set:')\r\n",
    "print(valid_df.labels.value_counts())\r\n",
    "print('Test Set:')\r\n",
    "print(test_df.labels.value_counts())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Set:\n",
      "1    2132\n",
      "0    1801\n",
      "Name: labels, dtype: int64\n",
      "Validation Set:\n",
      "1    243\n",
      "0    194\n",
      "Name: labels, dtype: int64\n",
      "Test Set:\n",
      "1    138\n",
      "0     92\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "X_train = train_df.file_paths.values\r\n",
    "y_train = train_df.labels.values\r\n",
    "X_valid = valid_df.file_paths.values\r\n",
    "y_valid = valid_df.labels.values\r\n",
    "X_test = test_df.file_paths.values\r\n",
    "y_test = test_df.labels.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "class BrainMRIDataset(Dataset):\r\n",
    "    def __init__(self, X, y):\r\n",
    "        self.X = X \r\n",
    "        self.y = y\r\n",
    "    \r\n",
    "    transform = T.Compose([\r\n",
    "        # T.ToPILImage(),\r\n",
    "        T.Resize((256, 256)),\r\n",
    "        T.ToTensor()])\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return (len(self.X))\r\n",
    "    \r\n",
    "    def __getitem__(self, i):\r\n",
    "        image = Image.open(self.X[i])\r\n",
    "        image = self.transform(image)\r\n",
    "        image = np.transpose(image, (2, 0, 1))\r\n",
    "        image = image.unsqueeze(0)\r\n",
    "        label = self.y[i]\r\n",
    "        return torch.tensor(image, dtype=torch.float), torch.tensor(label, dtype=torch.long)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "train_data = BrainMRIDataset(X_train, y_train)\r\n",
    "test_data = BrainMRIDataset(X_test, y_test)\r\n",
    "\r\n",
    "trainloader = DataLoader(train_data, batch_size=32, shuffle=True)\r\n",
    "testloader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "def model(pretrained, requires_grad):\r\n",
    "    model = tv.models.resnet50(progress=True, pretrained=pretrained)\r\n",
    "    # freeze hidden layers\r\n",
    "    if requires_grad == False:\r\n",
    "        for param in model.parameters():\r\n",
    "            param.requires_grad = False\r\n",
    "    # train the hidden layers\r\n",
    "    elif requires_grad == True:\r\n",
    "        for param in model.parameters():\r\n",
    "            param.requires_grad = True\r\n",
    "    # make the classification layer learnable\r\n",
    "    model.fc = nn.Linear(2048, 2)\r\n",
    "    return model\r\n",
    "model = model(pretrained=True, requires_grad=False).to(device)\r\n",
    "# print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "# optimizer\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.0005)\r\n",
    "# loss function\r\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "#validation function\r\n",
    "def validate(model, test_dataloader):\r\n",
    "    print('Validating')\r\n",
    "    model.eval()\r\n",
    "    val_running_loss = 0.0\r\n",
    "    val_running_correct = 0\r\n",
    "    with torch.no_grad():\r\n",
    "        for i, data in tqdm(enumerate(test_dataloader), total=int(len(test_data)/test_dataloader.batch_size)):\r\n",
    "            data, target = data[0].to(device), data[1].to(device)\r\n",
    "            outputs = model(data)\r\n",
    "            loss = criterion(outputs, target)\r\n",
    "            \r\n",
    "            val_running_loss += loss.item()\r\n",
    "            _, preds = torch.max(outputs.data, 1)\r\n",
    "            val_running_correct += (preds == target).sum().item()\r\n",
    "        \r\n",
    "        val_loss = val_running_loss/len(test_dataloader.dataset)\r\n",
    "        val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\r\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\r\n",
    "        \r\n",
    "        return val_loss, val_accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "# training function\r\n",
    "def fit(model, train_dataloader):\r\n",
    "    print('Training')\r\n",
    "    model.train()\r\n",
    "    train_running_loss = 0.0\r\n",
    "    train_running_correct = 0\r\n",
    "    for i, data in tqdm(enumerate(train_dataloader), total=int(len(train_data)/train_dataloader.batch_size)):\r\n",
    "        data, target = data[0].to(device), data[1].to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        outputs = model(data)\r\n",
    "        loss = criterion(outputs, target)\r\n",
    "        train_running_loss += loss.item()\r\n",
    "        _, preds = torch.max(outputs.data, 1)\r\n",
    "        train_running_correct += (preds == target).sum().item()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "    train_loss = train_running_loss/len(train_dataloader.dataset)\r\n",
    "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\r\n",
    "    \r\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}\")\r\n",
    "    \r\n",
    "    return train_loss, train_accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "train_loss , train_accuracy = [], []\r\n",
    "val_loss , val_accuracy = [], []\r\n",
    "start = time.time()\r\n",
    "\r\n",
    "for epoch in range(50):\r\n",
    "    print(f\"Epoch {epoch+1} of 50\")\r\n",
    "    train_epoch_loss, train_epoch_accuracy = fit(model, trainloader)\r\n",
    "    val_epoch_loss, val_epoch_accuracy = validate(model, testloader)\r\n",
    "    train_loss.append(train_epoch_loss)\r\n",
    "    train_accuracy.append(train_epoch_accuracy)\r\n",
    "    val_loss.append(val_epoch_loss)\r\n",
    "    val_accuracy.append(val_epoch_accuracy)\r\n",
    "\r\n",
    "end = time.time()\r\n",
    "print(f\"{(end-start)/60:.3f} minutes\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/122 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 of 50\n",
      "Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-152-ff6b1f4abbc2>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
      "  0%|          | 0/122 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 5-dimensional input of size [32, 1, 256, 3, 256] instead",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-a77c12bc9d78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch+1} of 50\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_epoch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_epoch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-157-76478a847ace>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, train_dataloader)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_running_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# See note [TorchScript super()]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 5-dimensional input of size [32, 1, 256, 3, 256] instead"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "59cbc324f0f5db281c8645694d449c7b22d2ee200ffe46629ff560bb12a1d6e5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}